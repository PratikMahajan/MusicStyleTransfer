{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Style Transfer using Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pratik Mahajan**<br/>\n",
    "Information Systems Department<br/>\n",
    "Northeastern University<br/>\n",
    "mahajan.pr@husky.neu.edu<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the recent success on Convolutional Neural Networks in style transfer  for images, we try to apply similar algorithms to experiment with style transfer in musical domain. In this paper, we use Convolutional Neural Network to transfer music style from one song to another. The music thus generated has features of both content as well as style song. The neural style algorithm[1] is used to celebrate the style and content from each song and recombine it to for a new music. In this paper we also state that how using Convolutional Neural Networks for audio processing results in imperfect output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last decade, Deep Neural Networks(DNNs) have gained a reputation to solve complex problems and emerged as a state of art solution for several (Artificial Intelligence) tasks like image classification, driverless cars, automation, etc \n",
    "\n",
    "After the success of Convolution Neural Networks in Image style transfer, we try to apply same technique to perform style transfer on music and look at its advantages and challanges involved. In this paper, we apply convolutional neural networks on spectogram of sound waves and determine weather it is a good decision whether to apply CNN on sound waves. \n",
    "\n",
    "It is difficult to define music style because of its multi-level and multi-modal character. The musical content can usually be denoted by the melody and style by harmonization. We also discuss current limitation of music style modelling and what can be done to make the transfer more accurate and feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Style Transfer: Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By separating and recombining music contents and music style, it is possible to generate new music that is both creative and human-like.In other words, we can still use our favorite data-driven algorithms but twist the constraints or optimizations in general by applying them seprately to different aspects (i.e., content and style) of music[2]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We convert the raw audio signal into its spectogram using the Short Time Fourier Transform. A spectogram is a 2D representation of a 1D signal, thus we can treat it as an image. It is equivalent to 1xT image with F number of channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We cannot use VGG-19 as 3x3 convolutions for out 1D problem, thus we have to use 1D convolutions. Thus we are training network with random weights. We are using only one layer of 4096 filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We reconstruct music file from the resultant spectogram. We do this using Griffin-Lim algorithm[11]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result as seen above is a new spectogram generated from the content and the style file. We convert this result spectogram to audio file using the Griffin-Lim algorithm. The result has features of both the content as well as the style audio file. As we increase the number of iterations, we can see that the features from both content and style become more prominant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result obtained out of music style transfer has features of both content as well as style audio. But, the different instruments are fused in the output, as a result, it is difficult to identify different instruments from the two different songs. As far as music style transfer is concerned, we were successful in transferring style from one song to another. The output sometimes generates melodious music and sometimes not. The second case happens typically when there is ensemble of multiple instruments in the song. This makes it seem like random instruments are playing from both the songs. But, when we use a single instrument song to transfer styles, we observe that the output is melodious to listen to and also there is good combination of style as well as content in the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems in Music Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a severe problem that music style is a fuzzy term and can refer to any aspect of music, ranging from high level features like tone and chord sequence to low level features like sounf texture and timbre[2]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Vs Sound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Images, the the concepts of content and style are intuitive. In images, we describe the objects present like trees, faces, animals, etc. This style is understood by colors, lighting, texture, edges, etc. However, the music is sementically abstract and is multi-dimentional in nature. Thus, musical content can mean different thing in different context. The musical content can usually be denoted by the melody and style by harmonization. But, we can also associate the lyrics with content and different melodies used to compose the song as the music style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axes of Spectrograms do not carry same meaning as images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In images, similar neighboring pixels can often be assumed to belong to the same visual object but in sound, frequencies are most often non-locally distributed on the spectrogram [3]. Periodic sounds are typically comprised of a fundamental frequency and a number of harmonics which are spaced apart by relationships dictated by the source of the sound. It is the mixture of these harmonics that determines the timbre of the sound [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sound is transparent, images are not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One challenge posed in the comparison between visual images and spectrograms is the fact that visual objects and sound events do not accumulate in the same manner. To use a visual analogy, one could say that sounds are always transparent [3] whereas most visual objects are opaque.\n",
    "\n",
    "When encountering a pixel of a certain color in an image, it can most often be assumed to belong to a single object. Discrete sound events do not separate into layers on a spectrogram: Instead, they all sum together into a distinct whole. That means that a particular observed frequency in a spectrogram cannot be assumed to belong to a single sound as the magnitude of that frequency could have been produced by any number of accumulated sounds or even by the complex interactions between sound waves such as phase cancellation. This makes it difficult to separate simultaneous sounds in spectrogram representations.[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spectral properties of sounds are non-local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In images, similar neighboring pixels can often be assumed to belong to the same visual object but in sound, frequencies are most often non-locally distributed on the spectrogram [3]. Periodic sounds are typically comprised of a fundamental frequency and a number of harmonics which are spaced apart by relationships dictated by the source of the sound. It is the mixture of these harmonics that determines the timbre of the sound [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Leon A. Gatys, Alexander S. Ecker, Matthias Bethge,\"A Neural Algorithm of Artistic Style\" <br/>\n",
    "[2] Shuqi Dai, Zheng Zhang, Gus G. Xia, \"Music Style Transfer: A Position Paper\"<br/>\n",
    "[3] L. Wyse, \"Audio Spectrogram Representations for Processing with Convolutional Neural Networks,\" vol. 1, no. 1, pp. 37–41, 2017.<br/>\n",
    "[4] \"What’s wrong with CNNs and spectrograms for audio processing?\" https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd<br/>\n",
    "[5] https://github.com/vadim-v-lebedev/audio_style_tranfer <br/>\n",
    "[6] https://github.com/Lasagne/Recipes/blob/master/examples/styletransfer/Art%20Style%20Transfer.ipynb <br/>\n",
    "[7] Extreme Style Machines: Using Random Neural Networks to Generate Textures https://nucl.ai/blog/extreme-style-machines/ <br/>\n",
    "[8] Ivan Ustyuzhaninov, Wieland Brendel, Leon A. Gatys, Matthias Bethge, \"Texture Synthesis Using Shallow Convolutional Networks with Random Filters\"<br/>\n",
    "[9] Kun He, Yan Wang, John Hopcroft, \"A Powerful Generative Model Using Random Weights for the Deep Image Representation\"<br/>\n",
    "[10] Shaun Barry, Youngmoo Kim, \"“Style” Transfer for Musical Audio Using Multiple Time-Frequency Representations \"\n",
    "[11] Dmitry Ulyanov and Vadim Lebedev, \"Audio texture synthesis and style transfer\"\n",
    "[12] D. Griffin, jae Lim, \"Signal estimation from modified short-time Fourier transform\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last updated: 08/14/2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
